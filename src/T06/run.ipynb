{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4363ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gymnasium import spaces\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# Only work with scipy 1.12.0\n",
    "# https://discourse.pymc.io/t/importerror-cannot-import-name-gaussian-from-scipy-signal/14170/3\n",
    "from scipy.signal import convolve, gaussian\n",
    "\n",
    "from T02_prototype_class.DE_IM_VRPTW_class2 import VRPTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9a1890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaEnv(gym.Env):\n",
    "    def __init__(self, vrp, interval_it=100):\n",
    "        # order = [\n",
    "        #     \"best_solution\",\n",
    "        #     \"F\",\n",
    "        #     \"CR\",\n",
    "        #     \"MG\",\n",
    "        #     \"percent_convergence\",\n",
    "        #     \"std_pop\",\n",
    "        #     \"count_total_iteration\",\n",
    "        # ]\n",
    "\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=np.array([-1e5, 0, 0, 0, 0, 0, 0], dtype=np.float64),\n",
    "            high=np.array([1e5, 1, 1, 1, 10, 1, 1], dtype=np.float64),\n",
    "            shape=(7,),  # 7 features\n",
    "            dtype=np.float64,\n",
    "        )\n",
    "\n",
    "        # self.observation_space = spaces.Dict(\n",
    "        #     # TODO: Check bounds\n",
    "        #     {\n",
    "        #         \"best_solution\": gym.spaces.Box(\n",
    "        #             low=0, high=1e6, shape=(1,), dtype=np.float64\n",
    "        #         ),\n",
    "        #         \"F\": gym.spaces.Box(low=0, high=1, shape=(1,), dtype=np.float64),\n",
    "        #         \"CR\": gym.spaces.Box(low=0, high=1, shape=(1,), dtype=np.float64),\n",
    "        #         \"MG\": gym.spaces.Box(low=0, high=1, shape=(1,), dtype=np.float64),\n",
    "        #         \"percent_convergence\": gym.spaces.Box(\n",
    "        #             low=0, high=10, shape=(1,), dtype=np.float64\n",
    "        #         ),\n",
    "        #         \"count_total_iteration\": gym.spaces.Box(\n",
    "        #             low=0, high=1e5, shape=(1,), dtype=np.int64\n",
    "        #         ),\n",
    "        #         \"std_pop\": gym.spaces.Box(low=0, high=10, shape=(1,), dtype=np.float64),\n",
    "        #         # \"DE_robust\": gym.spaces.Box(\n",
    "        #         #     low=0, high=1, shape=(1,), dtype=np.float64\n",
    "        #         # ),\n",
    "        #     }\n",
    "        # )\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(13)\n",
    "        self.vrp = vrp\n",
    "        self.interval_it = interval_it\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"Convert internal state to observation format.\n",
    "\n",
    "        Returns:\n",
    "            dict: Observation\n",
    "        \"\"\"\n",
    "        state = self.vrp.get_current_state()\n",
    "\n",
    "        obs = np.array(\n",
    "            [\n",
    "                np.float64(state[\"best_solution\"]),\n",
    "                np.float64(state[\"F\"]),\n",
    "                np.float64(state[\"CR\"]),\n",
    "                np.float64(state[\"MG\"]),\n",
    "                np.float64(state[\"percent_convergence\"]),\n",
    "                np.float64(state[\"std_pop\"]),\n",
    "                np.float64(state[\"count_total_iteration\"]),\n",
    "            ],\n",
    "            dtype=np.float64,\n",
    "        )\n",
    "\n",
    "        # obs = {\n",
    "        #     \"best_solution\": np.array([raw_obs[\"best_solution\"]], dtype=np.float64),\n",
    "        #     \"F\": np.array([raw_obs[\"F\"]], dtype=np.float64),\n",
    "        #     \"CR\": np.array([raw_obs[\"CR\"]], dtype=np.float64),\n",
    "        #     \"MG\": np.array([raw_obs[\"MG\"]], dtype=np.float64),\n",
    "        #     \"percent_convergence\": np.array(\n",
    "        #         [raw_obs[\"percent_convergence\"]], dtype=np.float64\n",
    "        #     ),\n",
    "        #     \"std_pop\": np.array([raw_obs[\"std_pop\"]], dtype=np.float64),\n",
    "        #     \"count_total_iteration\": np.array(\n",
    "        #         [raw_obs[\"count_total_iteration\"]], dtype=np.int64\n",
    "        #     ),\n",
    "        #     # \"DE_robust\": np.array([raw_obs[\"DE_robust\"]], dtype=np.float64) if needed\n",
    "        # }\n",
    "        return obs\n",
    "\n",
    "    def _get_info(self):\n",
    "        # TODO: We might want to see more stuff here.\n",
    "        \"\"\"Compute auxiliary information for debugging.\n",
    "\n",
    "        Returns:\n",
    "            dict: Info in addition to the observation\n",
    "        \"\"\"\n",
    "        return {**self.vrp.get_current_state()}\n",
    "\n",
    "    def reset(self, seed=None, options=None, **kwargs):\n",
    "        self.vrp.reset()\n",
    "        super().reset(seed=seed)\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # Expect action to be a number between 0 and 12\n",
    "        actionMap = {\n",
    "            0: \"F0CR0\",\n",
    "            1: \"F+CR0\",\n",
    "            2: \"F-CR0\",\n",
    "            3: \"F0CR+\",\n",
    "            4: \"F0CR-\",\n",
    "            5: \"F+CR+\",\n",
    "            6: \"F+CR-\",\n",
    "            7: \"F-CR+\",\n",
    "            8: \"F-CR-\",\n",
    "            9: \"MG0\",\n",
    "            10: \"MG10\",\n",
    "            11: \"MG25\",\n",
    "            12: \"MG50\",\n",
    "        }\n",
    "        # Check action and action map\n",
    "        if actionMap.get(action) is None:\n",
    "            raise Exception(\"Invalid action\")\n",
    "        self.vrp.action(actionMap[action])\n",
    "        self.vrp.evolve(n_iteration=self.interval_it)\n",
    "\n",
    "        reward = self.vrp.get_reward()\n",
    "        if self.vrp.calc_is_exceed_max_iteration():\n",
    "            terminated = True\n",
    "            truncated = True\n",
    "        else:\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        return observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff12079",
   "metadata": {},
   "outputs": [],
   "source": [
    "MetaEnv(vrp=None).observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd1cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file\n",
    "distance = (\n",
    "    pd.read_csv(\n",
    "        r\"../Source/solomon_data/solomon25_csv/distance_matrix/c101distanceMatrix.csv\"\n",
    "    )\n",
    "    .fillna(9999999)\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "df_vehicle = (\n",
    "    pd.read_csv(r\"../Source/solomon_data/solomon25_csv/data_info/c101dataInfo.csv\")\n",
    "    .iloc[:, :2]\n",
    "    .to_numpy(dtype=int)\n",
    ")\n",
    "vehicle = df_vehicle[0]\n",
    "\n",
    "df_101 = pd.read_csv(\n",
    "    r\"../Source/solomon_data/solomon25_csv/customers/c101customers.csv\"\n",
    ").iloc[:, 3:]\n",
    "demand = df_101.iloc[:, 0].to_numpy()\n",
    "readyTime = df_101.iloc[:, 1].to_numpy()\n",
    "dueDate = df_101.iloc[:, 2].to_numpy()\n",
    "serviceTime = df_101.iloc[:, -1].to_numpy()\n",
    "\n",
    "kwargs = {\n",
    "    \"distance\": distance,\n",
    "    \"demand\": demand,\n",
    "    \"readyTime\": readyTime,\n",
    "    \"dueDate\": dueDate,\n",
    "    \"serviceTime\": serviceTime,\n",
    "    \"vehicle\": vehicle,\n",
    "}\n",
    "\n",
    "dimensions = len(distance) - 1 + vehicle[0]\n",
    "interval_it = 100\n",
    "population_size = 100\n",
    "bounds = np.array([[0, 1]] * dimensions)\n",
    "F_rate = 0.5\n",
    "CR_rate = 0.5\n",
    "MG_rate = 0.5\n",
    "\n",
    "\n",
    "vrp = VRPTW(\n",
    "    population_size=population_size,\n",
    "    dimensions=dimensions,\n",
    "    bounds=bounds,\n",
    "    distance=distance,\n",
    "    demand=demand,\n",
    "    readyTime=readyTime,\n",
    "    dueDate=dueDate,\n",
    "    serviceTime=serviceTime,\n",
    "    vehicle=vehicle,\n",
    "    percent_convergence_lookback_it=interval_it,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1effa14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vrp.reset()\n",
    "vrp.get_current_state()\n",
    "# vrp.evolve(n_iteration=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a69044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix the environment to pass the checks\n",
    "# UserWarning: WARN: The obs returned by the `step()` method is not within the observation space.\n",
    "#   logger.warn(f\"{pre} is not within the observation space.\")\n",
    "# Environment has issues: Deterministic step observations are not equivalent for the same seed and action\n",
    "\n",
    "\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "\n",
    "env = MetaEnv(vrp=vrp, interval_it=100)\n",
    "\n",
    "# This will catch many common issues\n",
    "# try:\n",
    "#     check_env(env)\n",
    "#     print(\"Environment passes all checks!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Environment has issues: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f33b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = MetaEnv(vrp=vrp, interval_it=100)\n",
    "\n",
    "# print(env.reset())\n",
    "# print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50e3785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.step(action=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd8048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "state_dim = 7\n",
    "n_actions = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34018f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.actor = nn.Linear(128, n_actions)\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, s):\n",
    "        x = F.relu(self.fc1(s))\n",
    "        logits = self.actor(x)\n",
    "        state_value = self.critic(x)\n",
    "        return logits, state_value\n",
    "\n",
    "\n",
    "model = ActorCritic()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a7edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(state):\n",
    "    \"\"\"\n",
    "    params: states: [batch, state_dim]\n",
    "    returns: probs: [batch, n_actions]\n",
    "    \"\"\"\n",
    "    state = torch.tensor(state, device=device, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(state)\n",
    "    action_probs = nn.functional.softmax(logits, -1).detach().numpy()[0]\n",
    "    action = np.random.choice(n_actions, p=action_probs)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8628043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(env, n_steps=20):\n",
    "    \"\"\"\n",
    "    Play a session and genrate a trajectory\n",
    "    returns: arrays of states, actions, rewards\n",
    "    \"\"\"\n",
    "    states, actions, rewards = [], [], []\n",
    "\n",
    "    # initialize the environment\n",
    "    s, _ = env.reset()\n",
    "\n",
    "    # generate n_steps of trajectory:\n",
    "    for t in range(n_steps):\n",
    "        print(f\"Trajectory step: {t}\")\n",
    "        # sample action based on action_probs\n",
    "        a = sample_action(np.array([s]))\n",
    "        next_state, r, done, _, _ = env.step(a)\n",
    "\n",
    "        # update arrays\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd00f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "s, _ = env.reset()\n",
    "a = sample_action(np.array([s]))\n",
    "a\n",
    "next_state, r, done, _, _ = env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad43f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rewards_to_go(rewards, gamma=0.99):\n",
    "    T = len(rewards)  # total number of individual rewards\n",
    "    # empty array to return the rewards to go\n",
    "    rewards_to_go = [0] * T\n",
    "    rewards_to_go[T - 1] = rewards[T - 1]\n",
    "\n",
    "    for i in range(T - 2, -1, -1):  # go from T-2 to 0\n",
    "        rewards_to_go[i] = gamma * rewards_to_go[i + 1] + rewards[i]\n",
    "\n",
    "    return rewards_to_go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "def train_one_episode(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    # get rewards to go\n",
    "    rewards_to_go = get_rewards_to_go(rewards, gamma)\n",
    "\n",
    "    # convert numpy array to torch tensors\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float)\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.long)\n",
    "    rewards_to_go = torch.tensor(rewards_to_go, device=device, dtype=torch.float)\n",
    "\n",
    "    # get action probabilities from states\n",
    "    logits, state_values = model(states)\n",
    "    probs = nn.functional.softmax(logits, -1)\n",
    "    log_probs = nn.functional.log_softmax(logits, -1)\n",
    "\n",
    "    log_probs_for_actions = log_probs[range(len(actions)), actions]\n",
    "\n",
    "    advantage = rewards_to_go - state_values.squeeze(-1)\n",
    "\n",
    "    # Compute loss to be minized\n",
    "    J = torch.mean(log_probs_for_actions * (advantage))\n",
    "    H = -(probs * log_probs).sum(-1).mean()\n",
    "\n",
    "    loss = -(J + entropy_coef * H)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return np.sum(rewards)  # to show progress on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f86cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = 10\n",
    "total_rewards = []\n",
    "mean_rewards = []\n",
    "total_actions = []\n",
    "\n",
    "for i in range(total_steps):\n",
    "    print(f\"Step {i+1}/{total_steps}\")\n",
    "    states, actions, rewards = generate_trajectory(env)\n",
    "    reward = train_one_episode(states, actions, rewards)\n",
    "    total_rewards.append(reward)\n",
    "    total_actions.append(actions)\n",
    "    if i != 0 and i % 100 == 0:\n",
    "        mean_reward = np.mean(total_rewards[-100:-1])\n",
    "        mean_rewards.append(mean_reward)  # Store mean reward\n",
    "        print(\"mean reward:%.3f\" % (mean_reward))\n",
    "        if mean_reward > 950:\n",
    "            break\n",
    "\n",
    "env.close()\n",
    "\n",
    "plt.plot(mean_rewards)\n",
    "plt.xlabel(\"Episode (x100)\")\n",
    "plt.ylabel(\"Mean Reward (last 100)\")\n",
    "plt.title(\"Mean Reward Progression\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc253602",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78392431",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_actions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-meta-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
